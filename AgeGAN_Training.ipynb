{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AgeGAN - Training",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmbN8ylP2bmB",
        "colab_type": "text"
      },
      "source": [
        "### **Generation and Manipulation of faces using Generative Adversarial Networks**\n",
        "*Project for CSCI 630 - Foundations of Artificial Intelligence* \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   [Aishwarya Rao](ar2711@rit.edu)\n",
        "*   [Rishabh Manish Sahlot](rs3655@rit.edu)\n",
        "*   [Anuj Kulkarni](ak8285@rit.edu)\n",
        "*   [Shweta Vijay Wahane](sw9910@rit.edu)\n",
        "\n",
        "The following notebook uses conditional  generative  ad-versarial  networks  to  generate  realistic  images  of faces based age."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r460evki2Ok8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import statements\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "import cv2\n",
        "import dlib \n",
        "import os \n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from tqdm import tqdm \n",
        "import pickle\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2v3N47y2Zau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dataset is stored in drive as a tar file, it is read and extracted here\n",
        "downloaded = drive.CreateFile({'id':\"19W6fziIc3pSywcgO6lL5tHZAynlV21RX\"})   \n",
        "downloaded.GetContentFile('data.tar.gz')\n",
        "!tar -xvf data.tar.gz  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTXzKDE537uZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Face recognition model to obtain embeddings from every face in dataset\n",
        "downloaded = drive.CreateFile({'id':\"1XLHHUD3qMEk2i8SEVoIOvgR3cDQCeR1q\"})   \n",
        "downloaded.GetContentFile('dlib_face_recognition_resnet_model_v1.dat.bz2')\n",
        "# Landmark generator from dlib to identify facial landmarks\n",
        "# Facial Landmarks are used by the dlib face recognition model\n",
        "downloaded = drive.CreateFile({'id':\"1Q7Bj4YRpIXU2WF5zrUbJe3AazUGi0lCt\"})   \n",
        "downloaded.GetContentFile('shape_predictor_68_face_landmarks.dat.bz2')\n",
        "#Extract these for use \n",
        "!bzip2 dlib_face_recognition_resnet_model_v1.dat.bz2 --decompress\n",
        "!bzip2 shape_predictor_68_face_landmarks.dat.bz2 --decompress"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfe7bvaI4Yw7",
        "colab_type": "text"
      },
      "source": [
        "### **DATA PREPARATION**\n",
        "\n",
        "---\n",
        "Extracting face landmarks, getting embeddings and storing arrays of faces, corresponding age and face embedding. These will be used by our GAN model at a later stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lamuvhoD42fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Image sizes are resized to 100\n",
        "IMG_SIZE = 100\n",
        "\n",
        "def get_age(filename):\n",
        "  '''\n",
        "  age is extracted from filename. \n",
        "  Filename format for the dataset is age_gender_race_timestamp\n",
        "  Function returns quantized age as an integer\n",
        "  '''\n",
        "  minage = 0\n",
        "  maxage = 116\n",
        "  return int(filename.split(\"_\")[0])//5\n",
        "\n",
        "def get_dlib_embeddings(image):\n",
        "  '''\n",
        "  Takes in image, (grayscale)\n",
        "  returns 128 dimension facial embedding using the dlib face recognition model\n",
        "  '''\n",
        "  \n",
        "  face_detector = dlib.get_frontal_face_detector()\n",
        "  shape_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "  face_recognition_model = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
        "  \n",
        "  #Detect face position\n",
        "  detected_faces = face_detector(image, 1)\n",
        "  #Extract landmarks for every face detected - in this case, only one face is present\n",
        "  shapes_faces = [shape_predictor(image, face) for face in detected_faces]\n",
        "  #Return embedding of the face using face recognition model\n",
        "  return [np.array(face_recognition_model.compute_face_descriptor(image, face_pose, 1)) for face_pose in shapes_faces]\n",
        "\n",
        "def load_data(path):\n",
        "  '''\n",
        "  Reads every image, extracts age, face embeddings and appends to array\n",
        "  '''\n",
        "  real_data = []\n",
        "  age_values = []\n",
        "  embeddings_face = []\n",
        "  for image in tqdm(os.listdir(path)):\n",
        "    image_array = cv2.imread(path+\"/\"+image)\n",
        "    age_values.append(get_age(image))\n",
        "    image_array = cv2.resize(image_array, (IMG_SIZE, IMG_SIZE))\n",
        "    real_data.append(image_array)\n",
        "    embeddings_face.append(get_dlib_embeddings(image_array))\n",
        "  return real_data, age_values, embeddings_face\n",
        "\n",
        "#Contains faces in real train data, corresponding age values and face embeddings\n",
        "real_train_data, age_values, embeddings = load_data('UTKFace')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyD5RH8S56Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Store to file for later use\n",
        "pickle_out = open(\"embeddings.pickle\",\"wb\")\n",
        "pickle.dump(embeddings, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"age.pickle\",\"wb\")\n",
        "pickle.dump(age_values, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"faces.pickle\",\"wb\")\n",
        "pickle.dump(real_train_data, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNFrBfX6F_f",
        "colab_type": "text"
      },
      "source": [
        "### **CONDITONAL GAN MODEL**\n",
        "\n",
        "\n",
        "---\n",
        "Construct generator, discriminator, optimizers and loss functions. \\\\\n",
        "Includes training function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCSvUsO_6KF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torch import autograd\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78bNJxKa7QTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preparing data for model's use\n",
        "#Getting data from pickle files (data includes age values, array of faces and face embeddings)\n",
        "downloaded = drive.CreateFile({'id':\"1ZX4v97jWA_5iZciY50HLCVyJzKpitr62\"})   #Prestored on cloud \n",
        "downloaded.GetContentFile('age.pickle')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1FPmDblDjK1W9fUTljV1MaEghV8nlpK8m\"})   #Prestored on cloud\n",
        "downloaded.GetContentFile('embeddings.pickle')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1G9cJXmKTPZPxzJfpJfM3bPh2NwEp5AdH\"})   # Prestored on cloud\n",
        "downloaded.GetContentFile('faces.pickle')\n",
        "\n",
        "agevalues = pickle.load(open(\"age.pickle\", \"rb\"))\n",
        "embeddings = pickle.load(open(\"embeddings.pickle\", \"rb\"))\n",
        "faces = pickle.load(open(\"faces.pickle\", \"rb\"))\n",
        "\n",
        "\n",
        "remove = []\n",
        "#Remove all images whose faces were not reconized by dlib - no facial embeddings\n",
        "for index, arr in enumerate(embeddings):\n",
        "  if len(arr)<1:\n",
        "    remove.append(index)\n",
        "#One hot encoding representation for ages \n",
        "agevalues =[age for j, age in enumerate(agevalues) if j not in remove]\n",
        "tempage = [[0]*24 for _ in range(len(agevalues))]\n",
        "for index, age in enumerate(agevalues):\n",
        "  tempage[index][age]=1\n",
        "agevalues = tempage\n",
        "\n",
        "embeddings =[embed for j, embed in enumerate(embeddings) if j not in remove]\n",
        "#Faces are of shape 64x64 and in grayscale to run model faster\n",
        "faces =[cv2.cvtColor(cv2.resize(face, (64,64)), cv2.COLOR_BGR2GRAY) for j, face in enumerate(faces) if j not in remove]\n",
        "#Convert to tensors for PyTorch\n",
        "age = torch.Tensor(np.asarray(agevalues))\n",
        "embeddings = np.asarray([arr[0] for arr in embeddings])\n",
        "embed = torch.from_numpy(embeddings)\n",
        "face = torch.Tensor(np.asarray(faces)/255.0)\n",
        "\n",
        "#Use dataloader to prevent using the entire dataset in memory at once\n",
        "dataset = torch.utils.data.TensorDataset(face, embed, age)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG6Xe59r7-SC",
        "colab_type": "text"
      },
      "source": [
        "**DISCRIMINATOR MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_N0YR8-77dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Conditional GAN discriminator code - takes in image and age label - concatenates them for prediction\n",
        "#Returns a prediction - 0 if real, 1 is fake\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(2,32,5,2), #2 channels - one for image, one for age label\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv2d(32,64,5,2),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv2d(64,128,5,2),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Flatten(1,-1),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(3200,1,False),\n",
        "        nn.Sigmoid() #squash output between 0 and 1\n",
        "        )\n",
        "    #embed age vector into a 64x64 value\n",
        "    self.lab = nn.Linear(24,64*64*1,False)\n",
        "\n",
        "  def forward(self, x, labels):\n",
        "    x = x.view(x.size(0),1,64,64).float() #shape - batchsize x 1 x imgsize x imgsize\n",
        "    c = self.lab(labels)\n",
        "    c = c.view(labels.size(0),1,64,64).float()\n",
        "    x = torch.cat([x, c], 1)\n",
        "    out = self.model(x)\n",
        "    return out.squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHa2Cimm8o8m",
        "colab_type": "text"
      },
      "source": [
        "**GENERATOR MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79k_V03N8mL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator component for conditional GAN - takes in face embedding and age condition as input\n",
        "#Produces a 64x64 grayscale image\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #Encode condition into a 1024 vector before feeding into convolutoion layers\n",
        "        self.encoding = nn.Sequential(nn.Linear(128+24, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True))\n",
        "        self.model = nn.Sequential( \n",
        "            nn.ConvTranspose2d(1024, 64*8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(64* 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64* 8, 64 * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64 * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( 64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64* 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( 64 * 2, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( 64, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, z, labels):\n",
        "        z = z.view(z.size(0), 128).float() #size - batchsize x 128\n",
        "        c = labels.view(labels.size(0),24).float() #size - batchsize x 24\n",
        "        x = torch.cat([z, c], 1)\n",
        "        x= x.view(x.size(0), 152) #after concat - batchsize x (128+24)\n",
        "        x = self.encoding(x.float()) #Encode condition before convolution model\n",
        "        x = x.view(x.size(0), 1024, 1,1)\n",
        "        out = self.model(x.float())\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-YQpXxo9Ky-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shift to GPU\n",
        "generator = Generator().cuda()\n",
        "discriminator = Discriminator().cuda()\n",
        "\n",
        "#Loss 1 - used for generator and discriminator based on discriminator's prediction - increase realisticness of image\n",
        "criterion = nn.BCELoss()\n",
        "#Loss 2 - used for generator to reduce errors in image generation - increase similarity to original image\n",
        "g_loss1 = torch.nn.L1Loss()\n",
        "#Both models use Adam Optimizer\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyrwcPDJ9f1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training of both generator and discriminator\n",
        "\n",
        "#generator training \n",
        "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion, embeddings, labels, real_images):\n",
        "    g_optimizer.zero_grad()\n",
        "    z = embeddings\n",
        "    fake_labels = labels\n",
        "    #generate fake imges from embeddings and age condition\n",
        "    fake_images = generator(z, fake_labels)\n",
        "    #get discriminator's prediction\n",
        "    validity = discriminator(fake_images, fake_labels)\n",
        "    #overall loss = discriminator loss + lambda * image similarity loss\n",
        "    g_loss =criterion(validity, Variable(torch.zeros(batch_size)).cuda()) + 100*g_loss1(fake_images, real_images.view(real_images.shape[0], 1, 64,64)).cuda()\n",
        "    #Propoage loss\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    return g_loss.data\n",
        "\n",
        "#discriminator training\n",
        "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, embeddings, labels):\n",
        "    d_optimizer.zero_grad()\n",
        "    # train with real images\n",
        "    real_validity = discriminator(real_images, labels)\n",
        "    real_loss = criterion(real_validity, Variable(torch.zeros(batch_size)).cuda())\n",
        "    # train with fake images\n",
        "    z = embeddings\n",
        "    fake_labels = labels\n",
        "    fake_images = generator(z, fake_labels)\n",
        "    fake_validity = discriminator(fake_images, fake_labels)\n",
        "    fake_loss = criterion(fake_validity, Variable(torch.ones(batch_size)).cuda())\n",
        "    \n",
        "    #discriminator loss = how far away from 0 fot real images + how far away from 1 for fake images\n",
        "    d_loss = real_loss + fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    return d_loss.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_5_ti-E-DWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Overall GAN training\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    print('Starting epoch {}...'.format(epoch))\n",
        "    #set generator to train mode\n",
        "    #by default, models are in train mode\n",
        "    generator.train()\n",
        "    #get data one batch at a time\n",
        "    for i, (images, faceembed, labels) in enumerate(data_loader):\n",
        "        #shift to gpu\n",
        "        real_images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "        faceembeddings = Variable(faceembed).cuda()\n",
        "        \n",
        "        batch_size = real_images.size(0)\n",
        "        #Train discriminator\n",
        "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
        "                                          generator, d_optimizer, criterion,\n",
        "                                          real_images,faceembeddings, labels)\n",
        "        #Train generator - since discriminator reaches 0 loss quickly, every step has more generator training\n",
        "        for _ in range(5):\n",
        "          #5 steps of generator to one step of discriminator\n",
        "          g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion, faceembeddings, labels, real_images)\n",
        "       \n",
        "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
        "    if epoch%10==0:\n",
        "      #switch to evaluate model mode to display results\n",
        "      generator.eval()\n",
        "      #Pick random image from dataset \n",
        "      index = random.choice(range(len(embed)))\n",
        "      z = embed[index].view(1, 128).cuda()\n",
        "      real = faces[index]\n",
        "      #Set random age as condition\n",
        "      labels = Variable(torch.LongTensor(np.zeros((1, 24)))).cuda()\n",
        "      labels[0][4]=1\n",
        "      sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
        "      sample_images= sample_images.resize(64,64)\n",
        "      #Display images\n",
        "      sample_images = np.asarray(sample_images, dtype=\"float32\")*255\n",
        "      plt.imshow(sample_images, cmap=\"gray\")\n",
        "      plt.show()\n",
        "      plt.imshow(real, cmap=\"gray\")\n",
        "      plt.show()\n",
        "      #Save generator model weights in drive to use later \n",
        "      torch.save(generator.state_dict(), \"/content/gdrive/My Drive/generator.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}